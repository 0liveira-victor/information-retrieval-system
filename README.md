# ü§ñ Sistema de Recupera√ß√£o de Informa√ß√µes (RAG) Local

Este projeto implementa um sistema de **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)**, permitindo responder a perguntas com base no conte√∫do de um documento PDF, utilizando ferramentas *open-source* rodando totalmente localmente (on-premise).

O sistema √© constru√≠do sobre a orquestra√ß√£o do **LangChain**, a efici√™ncia do banco de dados vetorial **FAISS**, e a capacidade de processamento do **Ollama** para Embeddings e LLM.

## ‚ú® Tecnologias Utilizadas

| Componente | Tecnologia | Fun√ß√£o no Projeto |
| :--- | :--- | :--- |
| **LLM & Embeddings** | **Ollama** | Executa o modelo `llama3` para gera√ß√£o de respostas e o `all-minilm` para vetoriza√ß√£o de textos. |
| **Orquestra√ß√£o** | **LangChain** | Gerencia o pipeline RAG (carregamento, divis√£o, busca e resposta). |
| **Vector Database** | **FAISS** | Armazena e realiza buscas r√°pidas nos vetores (embeddings) do documento PDF. |
| **Processamento PDF** | **PyPDFLoader** | Respons√°vel por ler e extrair o texto do arquivo de entrada. |

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### 1. Pr√©-requisitos

Para rodar o projeto, voc√™ deve ter instalado:
1.  **Ollama:** O servi√ßo deve estar instalado e **ativo** (em `http://localhost:11434`).
2.  **Python:** Vers√£o 3.10 ou superior.
3.  **Arquivo de Dados:** Um arquivo PDF nomeado **`base_doc.pdf`** na raiz do projeto.

### 2. Configura√ß√£o de Modelos (Ollama)

Certifique-se de que os modelos de LLM e Embeddings foram baixados no seu Ollama:

```bash
ollama pull llama3
ollama pull all-minilm
```

### 3. Configura√ß√£o do Ambiente Python

#### 1. Crie e Ative a Venv:

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
.\venv\Scripts\activate   # Windows
```

#### 1. Instale as Depend√™ncias:

```bash
pip install -r requirements.txt
```

### 4. Execute o Sistema

```bash
python main.py
```

V√≠deo demonstrativo: [Link para o v√≠deo](https://www.loom.com/share/a547534677fc48e6b894978bd71a1345)