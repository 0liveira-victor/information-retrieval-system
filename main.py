from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters.character import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_classic.chains import RetrievalQA

# 1ï¸âƒ£ Caminho do PDF
CAMINHO_PDF = "./base_doc.pdf"  # coloque seu arquivo PDF aqui

# 2ï¸âƒ£ Carregar o PDF
print("ğŸ“„ Carregando documento PDF...")
loader = PyPDFLoader(CAMINHO_PDF)
docs = loader.load()

# 3ï¸âƒ£ Dividir o texto em partes menores
print("âœ‚ï¸ Dividindo texto em chunks...")
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)

# 4ï¸âƒ£ Gerar embeddings com Ollama
print("ğŸ§© Gerando embeddings com Ollama...")
embedding_model = OllamaEmbeddings(model="all-minilm")

# 5ï¸âƒ£ Criar o banco de dados vetorial FAISS
print("ğŸ’¾ Criando banco vetorial com FAISS...")
vectorstore = FAISS.from_documents(chunks, embedding_model)

# 6ï¸âƒ£ Configurar o modelo de linguagem para responder perguntas
llm = OllamaLLM(model="llama3")

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 7ï¸âƒ£ Loop de perguntas
print("\nâœ… Sistema pronto! FaÃ§a suas perguntas sobre o conteÃºdo do PDF.")
print("Digite 'sair' para encerrar.\n")

while True:
    pergunta = input("â“ Pergunta: ")
    if pergunta.lower() in ["sair", "exit", "quit"]:
        break

    print("\nğŸ’¬ Gerando resposta...")

    prompt_em_portugues = (
        "Responda a esta pergunta baseando-se apenas no contexto fornecido e **sempre no idioma PortuguÃªs do Brasil**: " 
        f"{pergunta}"
    )

    resposta = qa.invoke({"query": prompt_em_portugues})

    # compatibilidade entre versÃµes: pode vir como "answer" ou "result"
    output = resposta.get("answer") or resposta.get("result") or resposta
    print("\nğŸ’¬ Resposta:", output)
    print("-" * 80)


